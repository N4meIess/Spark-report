import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from collections import defaultdict
from sklearn.metrics import accuracy_score


# 1. 字向量工具
class WordVectorizer:
    def __init__(self, embedding_dim=120):
        self.embedding_dim = embedding_dim
        self.char2idx = {}  # 字到索引的映射
        self.idx2char = []  # 索引到字的映射
        self.embedding_matrix = None  # 字向量矩阵
        self.pad_char = '<PAD>'  # 填充符
        self.unk_char = '<UNK>'  # 未登录字

    def fit(self, corpus_path):
        """从语料中构建字表并随机初始化向量"""
        # 1. 统计语料中所有字
        char_counts = defaultdict(int)
        try:
            with open(corpus_path, 'r', encoding='utf-8') as f:
                line_count = 0
                for line in f:
                    line_count += 1
                    line = line.strip()
                    # 跳过表头（如果存在）
                    if line_count == 1 and 'sentence' in line.lower() and 'label' in line.lower():
                        continue
                    for char in line:
                        if char:  # 跳过空字符
                            char_counts[char] += 1
                print(f"✅ 成功加载语料，共处理 {line_count} 行文本")
                if line_count == 0:
                    raise ValueError("语料文件为空，请检查文件内容")
        except Exception as e:
            print(f"❌ 语料加载失败: {str(e)}")
            raise

        # 2. 构建字表（添加特殊符号）
        self.idx2char = [self.pad_char, self.unk_char]
        self.char2idx = {self.pad_char: 0, self.unk_char: 1}
        for char in char_counts:
            self.char2idx[char] = len(self.idx2char)
            self.idx2char.append(char)
        print(f"✅ 构建字表完成，字表大小: {len(self.idx2char)} 个字")

        # 3. 随机初始化字向量矩阵（均值0，标准差0.1）
        vocab_size = len(self.idx2char)
        self.embedding_matrix = np.random.normal(
            loc=0.0,
            scale=0.1,
            size=(vocab_size, self.embedding_dim)
        ).astype(np.float32)
        # 填充符向量初始化为全0
        self.embedding_matrix[0] = np.zeros(self.embedding_dim, dtype=np.float32)
        print(f"✅ 字向量矩阵初始化完成，维度: {self.embedding_matrix.shape}")

    def get_vector(self, char):
        """获取单个字的向量"""
        idx = self.char2idx.get(char, self.char2idx[self.unk_char])
        return self.embedding_matrix[idx]

    def save(self, save_dir):
        """保存字表和向量矩阵"""
        try:
            os.makedirs(save_dir, exist_ok=True)
            # 保存字表
            char2idx_path = os.path.join(save_dir, 'char2idx.npy')
            np.save(char2idx_path, self.char2idx)
            # 保存向量矩阵
            embedding_path = os.path.join(save_dir, 'embedding_matrix.npy')
            np.save(embedding_path, self.embedding_matrix)

            # 验证保存结果
            if not os.path.exists(char2idx_path) or not os.path.exists(embedding_path):
                raise IOError("保存文件失败，文件未生成")
            print(f"✅ 字向量文件已保存至: {save_dir}")
        except Exception as e:
            print(f"❌ 保存字向量失败: {str(e)}")
            raise

    def load(self, save_dir):
        """加载字表和向量矩阵"""
        try:
            # 检查目录是否存在
            if not os.path.exists(save_dir):
                raise FileNotFoundError(f"字向量目录不存在: {save_dir}")

            # 加载字表
            char2idx_path = os.path.join(save_dir, 'char2idx.npy')
            self.char2idx = np.load(char2idx_path, allow_pickle=True).item()

            # 加载向量矩阵
            embedding_path = os.path.join(save_dir, 'embedding_matrix.npy')
            self.embedding_matrix = np.load(embedding_path)

            # 构建索引到字的映射
            self.idx2char = [k for k, v in sorted(self.char2idx.items(), key=lambda x: x[1])]
            self.embedding_dim = self.embedding_matrix.shape[1]

            print(f"✅ 成功加载字向量，字表大小: {len(self.char2idx)}，向量维度: {self.embedding_dim}")
        except Exception as e:
            print(f"❌ 加载字向量失败: {str(e)}")
            raise


# 2. GLUE数据集加载类（以SST-2为例）
class GLUEDataset(Dataset):
    def __init__(self, tsv_path, vectorizer, max_seq_len=50, task='sst2'):

        self.vectorizer = vectorizer
        self.max_seq_len = max_seq_len
        self.task = task
        self.texts, self.labels = self._load_tsv(tsv_path)
        print(f"✅ 加载{tsv_path}完成，样本数: {len(self.texts)}")

    def _load_tsv(self, tsv_path):
        texts = []
        labels = []
        try:
            with open(tsv_path, 'r', encoding='utf-8') as f:
                header = next(f)  # 跳过表头
                for line_num, line in enumerate(f, 2):  # 从2开始计数（表头是1）
                    parts = line.strip().split('\t')
                    if self.task == 'sst2':
                        if len(parts) < 2:
                            print(f"⚠️ 跳过不完整行（行号{line_num}）: {line.strip()}")
                            continue
                        text = parts[0]
                        label = int(parts[1])
                        texts.append(text)
                        labels.append(label)
                return texts, labels
        except Exception as e:
            print(f"❌ 加载数据集失败: {str(e)}")
            raise

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        # 文本转字索引序列（截断/填充）
        char_ids = []
        for char in text[:self.max_seq_len]:  # 截断长文本
            # 使用vectorizer的unk_char属性
            char_ids.append(
                self.vectorizer.char2idx.get(char, self.vectorizer.char2idx[self.vectorizer.unk_char])
            )
        # 填充到max_seq_len
        padding_len = max(0, self.max_seq_len - len(char_ids))
        char_ids += [self.vectorizer.char2idx[self.vectorizer.pad_char]] * padding_len

        # 转Tensor
        char_ids = torch.tensor(char_ids, dtype=torch.long)
        label = torch.tensor(label, dtype=torch.long)
        return char_ids, label


# 3. 文本分类模型（TextCNN）
class TextCNN(nn.Module):
    def __init__(self, embedding_dim, vocab_size, num_labels,
                 num_filters=100, filter_sizes=[3, 4, 5], dropout=0.5, vectorizer=None):
        super(TextCNN, self).__init__()
        # 字向量嵌入层
        self.embedding = nn.Embedding(
            vocab_size,
            embedding_dim,
            padding_idx=0  # 临时值，后续会更新
        )

        # 加载预训练字向量
        if vectorizer is not None:
            self.embedding.weight.data.copy_(torch.from_numpy(vectorizer.embedding_matrix))
            # 设置padding_idx并确保其向量为全0
            self.pad_idx = vectorizer.char2idx['<PAD>']
            self.embedding.padding_idx = self.pad_idx
            self.embedding.weight.data[self.pad_idx] = torch.zeros(embedding_dim)
            print(f"✅ 加载预训练字向量，padding索引: {self.pad_idx}")

        # 多尺寸卷积核
        self.convs = nn.ModuleList([
            nn.Conv2d(
                in_channels=1,
                out_channels=num_filters,
                kernel_size=(fs, embedding_dim)
            )
            for fs in filter_sizes
        ])

        # dropout层
        self.dropout = nn.Dropout(dropout)
        # 分类层 - 输入维度是滤波器数量 × 滤波器尺寸数量
        self.fc = nn.Linear(num_filters * len(filter_sizes), num_labels)

    def forward(self, x):
        # x: [batch_size, max_seq_len]
        x = self.embedding(x).unsqueeze(1)  # [batch_size, 1, max_seq_len, embedding_dim]

        # 卷积 + 池化
        pooled_outputs = []
        for conv in self.convs:
            conv_out = torch.relu(conv(x))  # [batch_size, num_filters, seq_len - fs + 1, 1]
            # 修正池化操作，确保输出维度正确
            pooled = torch.max_pool2d(conv_out, kernel_size=(conv_out.size(2), 1)).squeeze(2).squeeze(2)
            pooled_outputs.append(pooled)  # 每个卷积核输出形状: [batch_size, num_filters]

        # 拼接特征 + dropout
        x = torch.cat(pooled_outputs, dim=1)  # [batch_size, num_filters * len(filter_sizes)]
        x = self.dropout(x)

        # 分类
        x = self.fc(x)
        return x


# 4. 训练与评估函数
def train_eval_model(model, train_loader, test_loader, epochs=10, lr=0.001, device='cuda'):
    model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        # 训练阶段
        model.train()
        train_loss = 0.0
        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * batch_x.size(0)

            # 打印进度
            if (batch_idx + 1) % 50 == 0:
                print(f"  批次 {batch_idx + 1}/{len(train_loader)}，当前批次损失: {loss.item():.4f}")

        # 验证阶段
        model.eval()
        y_true, y_pred = [], []
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                _, preds = torch.max(outputs, 1)
                y_true.extend(batch_y.cpu().numpy())
                y_pred.extend(preds.cpu().numpy())

        # 计算指标
        acc = accuracy_score(y_true, y_pred)
        print(f"\nEpoch {epoch + 1}/{epochs}")
        print(f"  训练损失: {train_loss / len(train_loader.dataset):.4f}")
        print(f"  测试准确率: {acc:.4f}\n" + "-" * 50)

    # 保存模型
    try:
        torch.save(model.state_dict(), 'textcnn_sst2.pth')
        print(f"✅ 模型已保存至: textcnn_sst2.pth")
    except Exception as e:
        print(f"❌ 模型保存失败: {str(e)}")


# 5. 预测函数（单句示例）
def predict_text(model, text, vectorizer, max_seq_len=50, device='cuda'):
    model.to(device)
    model.eval()

    # 文本转字索引序列（截断/填充）
    char_ids = []
    for char in text[:max_seq_len]:
        char_ids.append(
            vectorizer.char2idx.get(char, vectorizer.char2idx[vectorizer.unk_char])
        )
    padding_len = max(0, max_seq_len - len(char_ids))
    char_ids += [vectorizer.char2idx[vectorizer.pad_char]] * padding_len

    # 转Tensor并预测
    char_ids = torch.tensor(char_ids, dtype=torch.long).unsqueeze(0).to(device)
    with torch.no_grad():
        output = model(char_ids)
        _, pred = torch.max(output, 1)
    return pred.item()


# 6. 主函数（运行入口）
def main():
    # 配置参数
    embedding_dim = 120
    max_seq_len = 50
    epochs = 10
    batch_size = 64
    lr = 0.001
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"使用设备: {device}\n" + "-" * 50)

    # 1. 准备字向量
    vectorizer_save_dir = 'word_vector_sst2'
    sst2_train_path = r'C:\Users\22715\PycharmProjects\pythonProject\Spark\glue\SST-2\train.tsv'

    # 验证语料文件是否存在
    if not os.path.exists(sst2_train_path):
        raise FileNotFoundError(
            f"❌ 语料文件不存在: {sst2_train_path}\n"
            f"请检查路径是否正确，注意文件名是否为'train.tsv'，路径分隔符是否正确"
        )

    # 强制重新训练并保存字向量
    print("=" * 50 + "\n开始训练字向量...")
    vectorizer = WordVectorizer(embedding_dim=embedding_dim)
    vectorizer.fit(sst2_train_path)
    vectorizer.save(vectorizer_save_dir)

    # 重新加载验证
    print("\n验证字向量加载...")
    vectorizer = WordVectorizer()
    vectorizer.load(vectorizer_save_dir)

    # 2. 加载GLUE数据集（SST-2）
    print("\n" + "=" * 50 + "\n开始加载数据集...")
    sst2_train_tsv = r'C:\Users\22715\PycharmProjects\pythonProject\Spark\glue\SST-2\train.tsv'
    sst2_dev_tsv = r'C:\Users\22715\PycharmProjects\pythonProject\Spark\glue\SST-2\dev.tsv'

    # 验证数据集文件
    if not os.path.exists(sst2_train_tsv):
        raise FileNotFoundError(f"❌ 训练集不存在: {sst2_train_tsv}")
    if not os.path.exists(sst2_dev_tsv):
        raise FileNotFoundError(f"❌ 验证集不存在: {sst2_dev_tsv}")

    # 加载数据集
    train_dataset = GLUEDataset(
        sst2_train_tsv,
        vectorizer,
        max_seq_len=max_seq_len,
        task='sst2'
    )
    test_dataset = GLUEDataset(
        sst2_dev_tsv,
        vectorizer,
        max_seq_len=max_seq_len,
        task='sst2'
    )
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # 3. 初始化TextCNN模型
    print("\n" + "=" * 50 + "\n初始化模型...")
    vocab_size = len(vectorizer.char2idx)
    num_labels = 2  # SST-2是二分类（0:负面，1:正面）
    model = TextCNN(
        embedding_dim=embedding_dim,
        vocab_size=vocab_size,
        num_labels=num_labels,
        num_filters=100,
        filter_sizes=[3, 4, 5],
        dropout=0.5,
        vectorizer=vectorizer
    )
    print("✅ 模型初始化完成")

    # 4. 训练模型
    print("\n" + "=" * 50 + "\n开始训练模型...")
    train_eval_model(model, train_loader, test_loader, epochs=epochs, lr=lr, device=device)

    # 5. 测试预测
    print("\n" + "=" * 50 + "\n测试预测...")
    test_texts = [
        "This movie is amazing and touching",
        "I hate this terrible film",
        "The acting was good but the story was boring"
    ]
    for text in test_texts:
        pred_label = predict_text(model, text, vectorizer, max_seq_len, device)
        sentiment = "正面" if pred_label == 1 else "负面"
        print(f"文本: {text}")
        print(f"预测结果: {pred_label} ({sentiment})\n")


if __name__ == "__main__":
    import matplotlib.pyplot as plt

    plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]

    # 运行主函数
    main()
